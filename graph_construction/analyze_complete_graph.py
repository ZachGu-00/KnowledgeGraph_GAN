import pickle
import networkx as nx
import json
from collections import Counter, defaultdict
import numpy as np

def load_and_analyze_complete_graph():
    """åŠ è½½å¹¶åˆ†æå®Œæ•´çš„çŸ¥è¯†å›¾è°±"""
    print("=== Complete Knowledge Graph Analysis ===")
    
    # åŠ è½½å›¾
    with open('graph/knowledge_graph.pkl', 'rb') as f:
        graph = pickle.load(f)
    
    # åŠ è½½å®ä½“å­—å…¸
    with open('graph/entity_dict.json', 'r', encoding='utf-8') as f:
        entity_dict = json.load(f)
    # Convert string keys back to int
    entity_dict = {int(k): v for k, v in entity_dict.items()}
    
    print(f"Graph loaded: {graph.number_of_nodes():,} nodes, {graph.number_of_edges():,} edges")
    
    # 1. åŸºæœ¬ç»Ÿè®¡
    analyze_basic_stats(graph, entity_dict)
    
    # 2. è¿é€šæ€§åˆ†æ
    analyze_connectivity(graph, entity_dict)
    
    # 3. èŠ‚ç‚¹ç±»å‹åˆ†æ
    analyze_node_types(graph, entity_dict)
    
    # 4. å…³ç³»ç±»å‹åˆ†æ
    analyze_relations(graph, entity_dict)
    
    # 5. å±•ç¤ºå…·ä½“ä¾‹å­
    show_graph_examples(graph, entity_dict)
    
    return graph, entity_dict

def analyze_basic_stats(graph, entity_dict):
    """åŸºæœ¬ç»Ÿè®¡åˆ†æ"""
    print(f"\n=== Basic Statistics ===")
    
    num_nodes = graph.number_of_nodes()
    num_edges = graph.number_of_edges()
    
    print(f"Nodes: {num_nodes:,}")
    print(f"Edges: {num_edges:,}")
    print(f"Average degree: {2*num_edges/num_nodes:.2f}")
    print(f"Graph density: {nx.density(graph):.6f}")
    
    # åº¦åˆ†å¸ƒ
    degrees = dict(graph.degree())
    in_degrees = dict(graph.in_degree())
    out_degrees = dict(graph.out_degree())
    
    print(f"\nDegree Statistics:")
    print(f"  Total degree - Min: {min(degrees.values())}, Max: {max(degrees.values())}, Avg: {sum(degrees.values())/len(degrees):.2f}")
    print(f"  In-degree - Min: {min(in_degrees.values())}, Max: {max(in_degrees.values())}, Avg: {sum(in_degrees.values())/len(in_degrees):.2f}")
    print(f"  Out-degree - Min: {min(out_degrees.values())}, Max: {max(out_degrees.values())}, Avg: {sum(out_degrees.values())/len(out_degrees):.2f}")

def analyze_connectivity(graph, entity_dict):
    """è¿é€šæ€§åˆ†æ"""
    print(f"\n=== Connectivity Analysis ===")
    
    # è½¬ä¸ºæ— å‘å›¾åˆ†æè¿é€šæ€§
    undirected = graph.to_undirected()
    
    # è¿é€šåˆ†é‡
    components = list(nx.connected_components(undirected))
    num_components = len(components)
    
    print(f"Connected components: {num_components:,}")
    
    if components:
        # æœ€å¤§è¿é€šåˆ†é‡
        largest_comp = max(components, key=len)
        largest_size = len(largest_comp)
        
        print(f"Largest connected component: {largest_size:,} nodes ({largest_size/graph.number_of_nodes()*100:.2f}%)")
        
        # è¿é€šåˆ†é‡å¤§å°åˆ†å¸ƒ
        comp_sizes = sorted([len(c) for c in components], reverse=True)
        print(f"Component size distribution (top 10): {comp_sizes[:10]}")
        
        # å­¤ç«‹èŠ‚ç‚¹
        isolated_nodes = [list(c)[0] for c in components if len(c) == 1]
        print(f"Isolated nodes: {len(isolated_nodes):,}")
        
        if isolated_nodes:
            print("Sample isolated nodes:")
            for node_id in isolated_nodes[:10]:
                name = entity_dict.get(node_id, f'Entity-{node_id}')
                print(f"  {name}")
    
    # å¼ºè¿é€šåˆ†é‡ï¼ˆæœ‰å‘å›¾ï¼‰
    strong_components = list(nx.strongly_connected_components(graph))
    print(f"Strongly connected components: {len(strong_components):,}")
    if strong_components:
        largest_strong = max(strong_components, key=len)
        print(f"Largest strongly connected component: {len(largest_strong):,} nodes")

def analyze_node_types(graph, entity_dict):
    """èŠ‚ç‚¹ç±»å‹åˆ†æ"""
    print(f"\n=== Node Type Analysis ===")
    
    # é‡æ–°åˆ†ç±»èŠ‚ç‚¹
    node_types = {
        'movie': [],
        'person': [],
        'genre': [],
        'year': [],
        'language': [],
        'tag': [],
        'other': []
    }
    
    # é¢„å®šä¹‰çš„ç±»å‹
    genres = {'drama', 'comedy', 'action', 'horror', 'thriller', 'romance', 'war', 
             'documentary', 'adventure', 'crime', 'fantasy', 'mystery', 'sci-fi',
             'western', 'animation', 'family', 'biography'}
    
    languages = {'english', 'french', 'spanish', 'german', 'italian', 'japanese',
                'chinese', 'russian', 'portuguese', 'korean', 'hindi'}
    
    # åˆ†ç±»æ‰€æœ‰èŠ‚ç‚¹
    for node_id in graph.nodes():
        name = entity_dict.get(node_id, '').lower()
        
        if name.isdigit() and 1900 <= int(name) <= 2030:
            node_types['year'].append((node_id, entity_dict[node_id]))
        elif name in genres:
            node_types['genre'].append((node_id, entity_dict[node_id]))
        elif name in languages:
            node_types['language'].append((node_id, entity_dict[node_id]))
        elif (' ' in name and len(name.split()) >= 2 and 
              any(word[0].isupper() for word in entity_dict[node_id].split())):
            node_types['person'].append((node_id, entity_dict[node_id]))
        elif len(name.split()) <= 3 and all(len(word) <= 10 for word in name.split()):
            node_types['tag'].append((node_id, entity_dict[node_id]))
        else:
            node_types['movie'].append((node_id, entity_dict[node_id]))
    
    print("Node type distribution:")
    for node_type, nodes in node_types.items():
        print(f"  {node_type}: {len(nodes):,}")
        if nodes and len(nodes) <= 20:  # å¦‚æœæ•°é‡å°‘ï¼Œæ˜¾ç¤ºæ‰€æœ‰
            examples = [name for _, name in nodes]
        else:  # å¦åˆ™æ˜¾ç¤ºå‰5ä¸ªä¾‹å­
            examples = [name for _, name in nodes[:5]]
        if examples:
            print(f"    Examples: {examples}")

def analyze_relations(graph, entity_dict):
    """å…³ç³»ç±»å‹åˆ†æ"""
    print(f"\n=== Relation Type Analysis ===")
    
    # ç»Ÿè®¡æ¯ç§å…³ç³»
    relations = [data.get('relation', 'unknown') for _, _, data in graph.edges(data=True)]
    relation_counts = Counter(relations)
    
    print("Relation distribution:")
    total_edges = len(relations)
    for relation, count in relation_counts.most_common():
        percentage = count / total_edges * 100
        print(f"  {relation}: {count:,} ({percentage:.1f}%)")
    
    # å±•ç¤ºæ¯ç§å…³ç³»çš„ä¾‹å­
    print(f"\nRelation examples:")
    relation_examples = defaultdict(list)
    
    for source, target, data in graph.edges(data=True):
        relation = data.get('relation', 'unknown')
        if len(relation_examples[relation]) < 3:  # æ¯ç§å…³ç³»æœ€å¤š3ä¸ªä¾‹å­
            source_name = entity_dict.get(source, f'Entity-{source}')
            target_name = entity_dict.get(target, f'Entity-{target}')
            relation_examples[relation].append((source_name, target_name))
    
    for relation, examples in sorted(relation_examples.items()):
        print(f"  {relation}:")
        for source_name, target_name in examples:
            print(f"    {source_name} --[{relation}]--> {target_name}")

def show_graph_examples(graph, entity_dict):
    """æ˜¾ç¤ºå›¾ç»“æ„ä¾‹å­"""
    print(f"\n=== Graph Structure Examples ===")
    
    # æ‰¾ä¸€äº›æœ‰è¶£çš„èŠ‚ç‚¹
    degrees = dict(graph.degree())
    
    # æŒ‰åº¦æ•°æ’åºæ‰¾åˆ°ä¸­å¿ƒèŠ‚ç‚¹
    top_nodes = sorted(degrees.items(), key=lambda x: x[1], reverse=True)
    
    examples = []
    for node_id, degree in top_nodes:
        name = entity_dict.get(node_id, f'Entity-{node_id}')
        if degree > 50 and len(examples) < 3:  # æ‰¾3ä¸ªé«˜åº¦èŠ‚ç‚¹ä½œä¸ºä¾‹å­
            examples.append((node_id, name, degree))
    
    for i, (node_id, name, degree) in enumerate(examples):
        print(f"\nExample {i+1}: {name} (Total degree: {degree})")
        print(f"  In-degree: {graph.in_degree(node_id)}, Out-degree: {graph.out_degree(node_id)}")
        
        # å…¥åº¦è¾¹ä¾‹å­
        in_edges = list(graph.in_edges(node_id, data=True))[:5]
        if in_edges:
            print("  Incoming relations:")
            for source, _, data in in_edges:
                source_name = entity_dict.get(source, f'Entity-{source}')
                relation = data.get('relation', 'unknown')
                print(f"    {source_name} --[{relation}]--> {name}")
        
        # å‡ºåº¦è¾¹ä¾‹å­
        out_edges = list(graph.out_edges(node_id, data=True))[:5]
        if out_edges:
            print("  Outgoing relations:")
            for _, target, data in out_edges:
                target_name = entity_dict.get(target, f'Entity-{target}')
                relation = data.get('relation', 'unknown')
                print(f"    {name} --[{relation}]--> {target_name}")

def save_final_stats():
    """ä¿å­˜æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
    with open('graph/knowledge_graph.pkl', 'rb') as f:
        graph = pickle.load(f)
    
    undirected = graph.to_undirected()
    components = list(nx.connected_components(undirected))
    
    stats = {
        'nodes': graph.number_of_nodes(),
        'edges': graph.number_of_edges(),
        'connected_components': len(components),
        'largest_component_size': len(max(components, key=len)) if components else 0,
        'isolated_nodes': len([c for c in components if len(c) == 1]),
        'average_degree': 2 * graph.number_of_edges() / graph.number_of_nodes(),
        'density': nx.density(graph)
    }
    
    with open('graph/final_stats.json', 'w') as f:
        json.dump(stats, f, indent=2)
    
    return stats

def main():
    try:
        graph, entity_dict = load_and_analyze_complete_graph()
        stats = save_final_stats()
        
        print(f"\n" + "="*60)
        print("COMPLETE KNOWLEDGE GRAPH BUILT SUCCESSFULLY!")
        print("="*60)
        print(f"ğŸ“Š FINAL STATISTICS:")
        print(f"   â€¢ Total nodes: {stats['nodes']:,}")
        print(f"   â€¢ Total edges: {stats['edges']:,}")
        print(f"   â€¢ Connected components: {stats['connected_components']:,}")
        print(f"   â€¢ Largest component: {stats['largest_component_size']:,} nodes ({stats['largest_component_size']/stats['nodes']*100:.1f}%)")
        print(f"   â€¢ Isolated nodes: {stats['isolated_nodes']:,}")
        print(f"   â€¢ Average degree: {stats['average_degree']:.2f}")
        print(f"   â€¢ Graph density: {stats['density']:.6f}")
        print(f"")
        print(f"ğŸ“ Files saved in graph/ folder:")
        print(f"   â€¢ knowledge_graph.pkl - Main graph file")
        print(f"   â€¢ entity_dict.json - Entity ID to name mapping")
        print(f"   â€¢ final_stats.json - Graph statistics")
        
    except Exception as e:
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()